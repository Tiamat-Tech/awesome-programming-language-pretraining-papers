# Recent Advances in Programming Language Pre-Trained Models (PL-PTMs)
Maintained by WANG Yue (wangyue2714@gmail.com). Last update on 2021/05/22.


[Learning and Evaluating Contextual Embedding of Source Code](https://arxiv.org/abs/2001.00059), \[[code](https://github.com/google-research/google-research/tree/master/cubert)\] ICML 2020 (CuBERT)

[CodeBERT:A Pre-Trained Model for Programming and Natural Languages](https://arxiv.org/abs/2002.08155), \[[code](https://github.com/microsoft/CodeBERT)\] EMNLP 2020 Findings, (CodeBERT)

[GraphCodeBERT: Pre-training Code Representations with Data Flow](https://arxiv.org/abs/2009.08366),  \[[code](https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT)\] ICLR 2021 (GraphCodeBERT)

[CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation](https://arxiv.org/abs/2102.04664),  \[[code](https://github.com/microsoft/CodeXGLUE)\] arXiv 2021/02

[Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333), \[[code](https://github.com/wasiahmad/PLBART)\] NAACL 2021 (PLBART)

[Unsupervised Translation of Programming Languages](https://arxiv.org/abs/2006.03511),  \[[code](https://github.com/facebookresearch/TransCoder)\] NeurIPS 2020 (TransCoder)

[IntelliCode Compose: Code Generation using Transformer](https://arxiv.org/abs/2005.08025), FSE 2020

[Exploring Software Naturalness through Neural Language Models](https://arxiv.org/abs/2006.12641), arXiv 2020/06 (C-BERT)

[PYMT5: multi-mode translation of natural language and PYTHON code with transformers](https://arxiv.org/abs/2010.03150), EMNLP 2020

[Multi-task Learning based Pre-trained Language Model for Code Completion](https://arxiv.org/abs/2012.14631), ASE 2020

[Contrastive Code Representation Learning](https://arxiv.org/abs/2007.04973), \[[code](https://github.com/parasj/contracode)\] arXiv 2020/07

[GN-Transformer: Fusing AST and Source Code information in Graph Networks](https://openreview.net/forum?id=XavM6v_q59q), openreview 2020/09

[Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks](https://github.com/IBM/Project_CodeNet/blob/main/ProjectCodeNet_NeurIPS2021.pdf) \[[code](https://github.com/IBM/Project_CodeNet)\] 

[DOBF: A Deobfuscation Pre-Training Objective for Programming Languages](https://arxiv.org/abs/2102.07492), arXiv 2021/02

[Language-Agnostic Representation Learning of Source Code from Structure and Context](https://arxiv.org/abs/2103.11318), \[[code](https://github.com/danielzuegner/code-transformer)\] ICLR 2021

[Generating Bug-Fixes Using Pretrained Transformers](https://arxiv.org/abs/2104.07896), arXiv 2021/04

[CodeTrans: Towards Cracking the Language of Siliconeâ€™s Code Through Self-Supervised Deep Learning and High Performance Computing](https://arxiv.org/abs/2104.02443), \[[code](https://github.com/agemagician/CodeTrans)\] arXiv 2021/04 (CodeTrans)

[How could Neural Networks understand Programs?](https://arxiv.org/pdf/2105.04297.pdf), \[[code](https://github.com/pdlan/OSCAR)\] ICML 2021 (OSCAR)



